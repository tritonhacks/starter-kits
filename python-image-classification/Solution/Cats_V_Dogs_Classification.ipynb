{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taQmAnXrfBLc"
      },
      "source": [
        "# 1. Loading data and setting up folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ2yt-kILVKl",
        "outputId": "381b2461-36dc-48e4-bdd5-37b47bf21fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-02-02 01:51:38--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.31.128, 173.194.210.128, 173.194.211.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.31.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   193MB/s    in 0.3s    \n",
            "\n",
            "2022-02-02 01:51:39 (193 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#This cell is purely for loading the data and setting the directories for convenience\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "-O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "#This loads in the dataset you'll be using for this project.\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')  #some of this data will be split for validation later\n",
        "testing_dir = os.path.join(base_dir, 'validation')  #this data will be reserved for testing despite the directory's name\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "testing_cats_dir = os.path.join(testing_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "testing_dogs_dir = os.path.join(testing_dir, 'dogs')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUiFmISjXW48",
        "outputId": "4773b7f9-b164-41cc-e231-7d254ff5633f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['dogs', 'cats']\n",
            "['cat.368.jpg', 'cat.578.jpg', 'cat.646.jpg', 'cat.483.jpg', 'cat.724.jpg', 'cat.587.jpg', 'cat.827.jpg', 'cat.267.jpg', 'cat.410.jpg', 'cat.737.jpg', 'cat.461.jpg', 'cat.48.jpg', 'cat.375.jpg', 'cat.582.jpg', 'cat.344.jpg', 'cat.31.jpg', 'cat.898.jpg', 'cat.469.jpg', 'cat.161.jpg', 'cat.268.jpg', 'cat.303.jpg', 'cat.677.jpg', 'cat.374.jpg', 'cat.676.jpg', 'cat.797.jpg', 'cat.929.jpg', 'cat.438.jpg', 'cat.54.jpg', 'cat.89.jpg', 'cat.339.jpg', 'cat.650.jpg', 'cat.637.jpg', 'cat.431.jpg', 'cat.598.jpg', 'cat.838.jpg', 'cat.590.jpg', 'cat.221.jpg', 'cat.292.jpg', 'cat.399.jpg', 'cat.265.jpg', 'cat.125.jpg', 'cat.632.jpg', 'cat.18.jpg', 'cat.763.jpg', 'cat.377.jpg', 'cat.823.jpg', 'cat.332.jpg', 'cat.274.jpg', 'cat.283.jpg', 'cat.69.jpg', 'cat.32.jpg', 'cat.974.jpg', 'cat.798.jpg', 'cat.231.jpg', 'cat.519.jpg', 'cat.840.jpg', 'cat.512.jpg', 'cat.406.jpg', 'cat.518.jpg', 'cat.530.jpg', 'cat.351.jpg', 'cat.854.jpg', 'cat.328.jpg', 'cat.489.jpg', 'cat.501.jpg', 'cat.385.jpg', 'cat.443.jpg', 'cat.259.jpg', 'cat.812.jpg', 'cat.448.jpg', 'cat.997.jpg', 'cat.834.jpg', 'cat.777.jpg', 'cat.1.jpg', 'cat.264.jpg', 'cat.966.jpg', 'cat.557.jpg', 'cat.488.jpg', 'cat.317.jpg', 'cat.355.jpg', 'cat.297.jpg', 'cat.729.jpg', 'cat.148.jpg', 'cat.542.jpg', 'cat.487.jpg', 'cat.681.jpg', 'cat.22.jpg', 'cat.864.jpg', 'cat.932.jpg', 'cat.705.jpg', 'cat.459.jpg', 'cat.889.jpg', 'cat.500.jpg', 'cat.837.jpg', 'cat.968.jpg', 'cat.610.jpg', 'cat.641.jpg', 'cat.155.jpg', 'cat.965.jpg', 'cat.962.jpg', 'cat.156.jpg', 'cat.36.jpg', 'cat.961.jpg', 'cat.279.jpg', 'cat.199.jpg', 'cat.64.jpg', 'cat.904.jpg', 'cat.628.jpg', 'cat.508.jpg', 'cat.682.jpg', 'cat.671.jpg', 'cat.996.jpg', 'cat.364.jpg', 'cat.112.jpg', 'cat.873.jpg', 'cat.881.jpg', 'cat.899.jpg', 'cat.710.jpg', 'cat.667.jpg', 'cat.572.jpg', 'cat.908.jpg', 'cat.711.jpg', 'cat.618.jpg', 'cat.652.jpg', 'cat.688.jpg', 'cat.645.jpg', 'cat.540.jpg', 'cat.153.jpg', 'cat.583.jpg', 'cat.565.jpg', 'cat.970.jpg', 'cat.130.jpg', 'cat.41.jpg', 'cat.329.jpg', 'cat.225.jpg', 'cat.614.jpg', 'cat.324.jpg', 'cat.927.jpg', 'cat.741.jpg', 'cat.354.jpg', 'cat.196.jpg', 'cat.825.jpg', 'cat.979.jpg', 'cat.94.jpg', 'cat.252.jpg', 'cat.526.jpg', 'cat.34.jpg', 'cat.695.jpg', 'cat.42.jpg', 'cat.380.jpg', 'cat.298.jpg', 'cat.429.jpg', 'cat.345.jpg', 'cat.21.jpg', 'cat.651.jpg', 'cat.547.jpg', 'cat.496.jpg', 'cat.781.jpg', 'cat.247.jpg', 'cat.472.jpg', 'cat.154.jpg', 'cat.245.jpg', 'cat.896.jpg', 'cat.749.jpg', 'cat.113.jpg', 'cat.389.jpg', 'cat.971.jpg', 'cat.442.jpg', 'cat.107.jpg', 'cat.330.jpg', 'cat.647.jpg', 'cat.802.jpg', 'cat.495.jpg', 'cat.934.jpg', 'cat.371.jpg', 'cat.424.jpg', 'cat.627.jpg', 'cat.0.jpg', 'cat.220.jpg', 'cat.175.jpg', 'cat.556.jpg', 'cat.362.jpg', 'cat.433.jpg', 'cat.505.jpg', 'cat.447.jpg', 'cat.182.jpg', 'cat.174.jpg', 'cat.128.jpg', 'cat.915.jpg', 'cat.249.jpg', 'cat.284.jpg', 'cat.736.jpg', 'cat.266.jpg', 'cat.947.jpg', 'cat.240.jpg', 'cat.891.jpg', 'cat.255.jpg', 'cat.50.jpg', 'cat.493.jpg', 'cat.509.jpg', 'cat.586.jpg', 'cat.139.jpg', 'cat.206.jpg', 'cat.850.jpg', 'cat.606.jpg', 'cat.482.jpg', 'cat.761.jpg', 'cat.619.jpg', 'cat.639.jpg', 'cat.420.jpg', 'cat.349.jpg', 'cat.134.jpg', 'cat.288.jpg', 'cat.506.jpg', 'cat.739.jpg', 'cat.689.jpg', 'cat.239.jpg', 'cat.60.jpg', 'cat.755.jpg', 'cat.312.jpg', 'cat.490.jpg', 'cat.195.jpg', 'cat.473.jpg', 'cat.634.jpg', 'cat.953.jpg', 'cat.218.jpg', 'cat.92.jpg', 'cat.895.jpg', 'cat.124.jpg', 'cat.536.jpg', 'cat.208.jpg', 'cat.480.jpg', 'cat.742.jpg', 'cat.516.jpg', 'cat.939.jpg', 'cat.226.jpg', 'cat.952.jpg', 'cat.238.jpg', 'cat.888.jpg', 'cat.900.jpg', 'cat.719.jpg', 'cat.281.jpg', 'cat.942.jpg', 'cat.114.jpg', 'cat.747.jpg', 'cat.299.jpg', 'cat.321.jpg', 'cat.914.jpg', 'cat.499.jpg', 'cat.973.jpg', 'cat.186.jpg', 'cat.909.jpg', 'cat.338.jpg', 'cat.191.jpg', 'cat.806.jpg', 'cat.360.jpg', 'cat.886.jpg', 'cat.675.jpg', 'cat.843.jpg', 'cat.350.jpg', 'cat.81.jpg', 'cat.940.jpg', 'cat.981.jpg', 'cat.844.jpg', 'cat.814.jpg', 'cat.765.jpg', 'cat.655.jpg', 'cat.116.jpg', 'cat.460.jpg', 'cat.160.jpg', 'cat.228.jpg', 'cat.820.jpg', 'cat.456.jpg', 'cat.600.jpg', 'cat.322.jpg', 'cat.466.jpg', 'cat.612.jpg', 'cat.2.jpg', 'cat.921.jpg', 'cat.581.jpg', 'cat.468.jpg', 'cat.326.jpg', 'cat.388.jpg', 'cat.835.jpg', 'cat.313.jpg', 'cat.699.jpg', 'cat.248.jpg', 'cat.178.jpg', 'cat.381.jpg', 'cat.286.jpg', 'cat.972.jpg', 'cat.422.jpg', 'cat.669.jpg', 'cat.819.jpg', 'cat.23.jpg', 'cat.865.jpg', 'cat.925.jpg', 'cat.26.jpg', 'cat.963.jpg', 'cat.166.jpg', 'cat.548.jpg', 'cat.815.jpg', 'cat.599.jpg', 'cat.735.jpg', 'cat.967.jpg', 'cat.664.jpg', 'cat.391.jpg', 'cat.49.jpg', 'cat.579.jpg', 'cat.481.jpg', 'cat.980.jpg', 'cat.109.jpg', 'cat.87.jpg', 'cat.818.jpg', 'cat.246.jpg', 'cat.717.jpg', 'cat.202.jpg', 'cat.209.jpg', 'cat.35.jpg', 'cat.862.jpg', 'cat.181.jpg', 'cat.450.jpg', 'cat.615.jpg', 'cat.331.jpg', 'cat.14.jpg', 'cat.187.jpg', 'cat.341.jpg', 'cat.285.jpg', 'cat.127.jpg', 'cat.319.jpg', 'cat.470.jpg', 'cat.912.jpg', 'cat.411.jpg', 'cat.696.jpg', 'cat.243.jpg', 'cat.757.jpg', 'cat.983.jpg', 'cat.270.jpg', 'cat.152.jpg', 'cat.665.jpg', 'cat.535.jpg', 'cat.673.jpg', 'cat.592.jpg', 'cat.57.jpg', 'cat.702.jpg', 'cat.441.jpg', 'cat.439.jpg', 'cat.532.jpg', 'cat.167.jpg', 'cat.210.jpg', 'cat.452.jpg', 'cat.817.jpg', 'cat.363.jpg', 'cat.306.jpg', 'cat.133.jpg', 'cat.810.jpg', 'cat.559.jpg', 'cat.913.jpg', 'cat.97.jpg', 'cat.277.jpg', 'cat.287.jpg', 'cat.142.jpg', 'cat.633.jpg', 'cat.976.jpg', 'cat.212.jpg', 'cat.394.jpg', 'cat.609.jpg', 'cat.408.jpg', 'cat.692.jpg', 'cat.305.jpg', 'cat.188.jpg', 'cat.120.jpg', 'cat.10.jpg', 'cat.39.jpg', 'cat.164.jpg', 'cat.392.jpg', 'cat.71.jpg', 'cat.333.jpg', 'cat.417.jpg', 'cat.784.jpg', 'cat.293.jpg', 'cat.754.jpg', 'cat.847.jpg', 'cat.658.jpg', 'cat.194.jpg', 'cat.829.jpg', 'cat.573.jpg', 'cat.607.jpg', 'cat.502.jpg', 'cat.43.jpg', 'cat.617.jpg', 'cat.27.jpg', 'cat.796.jpg', 'cat.9.jpg', 'cat.611.jpg', 'cat.923.jpg', 'cat.987.jpg', 'cat.985.jpg', 'cat.789.jpg', 'cat.343.jpg', 'cat.316.jpg', 'cat.271.jpg', 'cat.179.jpg', 'cat.792.jpg', 'cat.229.jpg', 'cat.680.jpg', 'cat.807.jpg', 'cat.560.jpg', 'cat.787.jpg', 'cat.867.jpg', 'cat.630.jpg', 'cat.103.jpg', 'cat.759.jpg', 'cat.816.jpg', 'cat.849.jpg', 'cat.770.jpg', 'cat.357.jpg', 'cat.744.jpg', 'cat.395.jpg', 'cat.766.jpg', 'cat.693.jpg', 'cat.991.jpg', 'cat.892.jpg', 'cat.684.jpg', 'cat.376.jpg', 'cat.165.jpg', 'cat.471.jpg', 'cat.402.jpg', 'cat.944.jpg', 'cat.588.jpg', 'cat.935.jpg', 'cat.334.jpg', 'cat.524.jpg', 'cat.808.jpg', 'cat.826.jpg', 'cat.858.jpg', 'cat.764.jpg', 'cat.437.jpg', 'cat.409.jpg', 'cat.564.jpg', 'cat.454.jpg', 'cat.746.jpg', 'cat.575.jpg', 'cat.327.jpg', 'cat.885.jpg', 'cat.234.jpg', 'cat.543.jpg', 'cat.698.jpg', 'cat.59.jpg', 'cat.291.jpg', 'cat.992.jpg', 'cat.706.jpg', 'cat.393.jpg', 'cat.289.jpg', 'cat.687.jpg', 'cat.359.jpg', 'cat.498.jpg', 'cat.785.jpg', 'cat.76.jpg', 'cat.204.jpg', 'cat.176.jpg', 'cat.67.jpg', 'cat.275.jpg', 'cat.788.jpg', 'cat.605.jpg', 'cat.568.jpg', 'cat.657.jpg', 'cat.821.jpg', 'cat.201.jpg', 'cat.828.jpg', 'cat.353.jpg', 'cat.256.jpg', 'cat.100.jpg', 'cat.916.jpg', 'cat.497.jpg', 'cat.771.jpg', 'cat.84.jpg', 'cat.988.jpg', 'cat.672.jpg', 'cat.462.jpg', 'cat.280.jpg', 'cat.337.jpg', 'cat.192.jpg', 'cat.678.jpg', 'cat.852.jpg', 'cat.595.jpg', 'cat.320.jpg', 'cat.520.jpg', 'cat.857.jpg', 'cat.173.jpg', 'cat.839.jpg', 'cat.522.jpg', 'cat.722.jpg', 'cat.77.jpg', 'cat.63.jpg', 'cat.11.jpg', 'cat.111.jpg', 'cat.799.jpg', 'cat.61.jpg', 'cat.541.jpg', 'cat.649.jpg', 'cat.135.jpg', 'cat.403.jpg', 'cat.640.jpg', 'cat.529.jpg', 'cat.875.jpg', 'cat.928.jpg', 'cat.117.jpg', 'cat.701.jpg', 'cat.423.jpg', 'cat.594.jpg', 'cat.642.jpg', 'cat.51.jpg', 'cat.553.jpg', 'cat.890.jpg', 'cat.773.jpg', 'cat.40.jpg', 'cat.421.jpg', 'cat.314.jpg', 'cat.948.jpg', 'cat.171.jpg', 'cat.906.jpg', 'cat.260.jpg', 'cat.977.jpg', 'cat.44.jpg', 'cat.860.jpg', 'cat.887.jpg', 'cat.964.jpg', 'cat.261.jpg', 'cat.597.jpg', 'cat.552.jpg', 'cat.783.jpg', 'cat.83.jpg', 'cat.775.jpg', 'cat.804.jpg', 'cat.920.jpg', 'cat.956.jpg', 'cat.707.jpg', 'cat.922.jpg', 'cat.426.jpg', 'cat.222.jpg', 'cat.833.jpg', 'cat.894.jpg', 'cat.938.jpg', 'cat.679.jpg', 'cat.848.jpg', 'cat.841.jpg', 'cat.379.jpg', 'cat.336.jpg', 'cat.853.jpg', 'cat.624.jpg', 'cat.731.jpg', 'cat.551.jpg', 'cat.168.jpg', 'cat.126.jpg', 'cat.978.jpg', 'cat.444.jpg', 'cat.732.jpg', 'cat.851.jpg', 'cat.55.jpg', 'cat.348.jpg', 'cat.159.jpg', 'cat.959.jpg', 'cat.121.jpg', 'cat.189.jpg', 'cat.585.jpg', 'cat.290.jpg', 'cat.132.jpg', 'cat.527.jpg', 'cat.504.jpg', 'cat.648.jpg', 'cat.415.jpg', 'cat.767.jpg', 'cat.430.jpg', 'cat.525.jpg', 'cat.428.jpg', 'cat.136.jpg', 'cat.131.jpg', 'cat.7.jpg', 'cat.361.jpg', 'cat.119.jpg', 'cat.683.jpg', 'cat.700.jpg', 'cat.106.jpg', 'cat.99.jpg', 'cat.513.jpg', 'cat.733.jpg', 'cat.751.jpg', 'cat.253.jpg', 'cat.622.jpg', 'cat.382.jpg', 'cat.752.jpg', 'cat.601.jpg', 'cat.625.jpg', 'cat.756.jpg', 'cat.453.jpg', 'cat.845.jpg', 'cat.250.jpg', 'cat.924.jpg', 'cat.12.jpg', 'cat.723.jpg', 'cat.726.jpg', 'cat.546.jpg', 'cat.145.jpg', 'cat.414.jpg', 'cat.809.jpg', 'cat.269.jpg', 'cat.475.jpg', 'cat.68.jpg', 'cat.740.jpg', 'cat.16.jpg', 'cat.960.jpg', 'cat.72.jpg', 'cat.791.jpg', 'cat.758.jpg', 'cat.563.jpg', 'cat.66.jpg', 'cat.768.jpg', 'cat.521.jpg', 'cat.491.jpg', 'cat.955.jpg', 'cat.193.jpg', 'cat.846.jpg', 'cat.774.jpg', 'cat.713.jpg', 'cat.511.jpg', 'cat.571.jpg', 'cat.104.jpg', 'cat.384.jpg', 'cat.591.jpg', 'cat.589.jpg', 'cat.436.jpg', 'cat.390.jpg', 'cat.102.jpg', 'cat.994.jpg', 'cat.372.jpg', 'cat.440.jpg', 'cat.730.jpg', 'cat.949.jpg', 'cat.158.jpg', 'cat.150.jpg', 'cat.570.jpg', 'cat.273.jpg', 'cat.750.jpg', 'cat.545.jpg', 'cat.4.jpg', 'cat.715.jpg', 'cat.342.jpg', 'cat.435.jpg', 'cat.53.jpg', 'cat.263.jpg', 'cat.17.jpg', 'cat.901.jpg', 'cat.566.jpg', 'cat.8.jpg', 'cat.569.jpg', 'cat.140.jpg', 'cat.510.jpg', 'cat.404.jpg', 'cat.241.jpg', 'cat.30.jpg', 'cat.748.jpg', 'cat.214.jpg', 'cat.47.jpg', 'cat.19.jpg', 'cat.180.jpg', 'cat.635.jpg', 'cat.503.jpg', 'cat.82.jpg', 'cat.419.jpg', 'cat.728.jpg', 'cat.278.jpg', 'cat.537.jpg', 'cat.122.jpg', 'cat.544.jpg', 'cat.70.jpg', 'cat.315.jpg', 'cat.636.jpg', 'cat.244.jpg', 'cat.257.jpg', 'cat.129.jpg', 'cat.620.jpg', 'cat.884.jpg', 'cat.918.jpg', 'cat.902.jpg', 'cat.871.jpg', 'cat.302.jpg', 'cat.434.jpg', 'cat.870.jpg', 'cat.177.jpg', 'cat.954.jpg', 'cat.869.jpg', 'cat.855.jpg', 'cat.149.jpg', 'cat.999.jpg', 'cat.373.jpg', 'cat.144.jpg', 'cat.340.jpg', 'cat.62.jpg', 'cat.294.jpg', 'cat.734.jpg', 'cat.45.jpg', 'cat.969.jpg', 'cat.79.jpg', 'cat.879.jpg', 'cat.690.jpg', 'cat.950.jpg', 'cat.718.jpg', 'cat.492.jpg', 'cat.800.jpg', 'cat.550.jpg', 'cat.515.jpg', 'cat.951.jpg', 'cat.697.jpg', 'cat.593.jpg', 'cat.674.jpg', 'cat.603.jpg', 'cat.643.jpg', 'cat.318.jpg', 'cat.531.jpg', 'cat.803.jpg', 'cat.37.jpg', 'cat.795.jpg', 'cat.233.jpg', 'cat.580.jpg', 'cat.866.jpg', 'cat.15.jpg', 'cat.824.jpg', 'cat.608.jpg', 'cat.721.jpg', 'cat.986.jpg', 'cat.507.jpg', 'cat.28.jpg', 'cat.984.jpg', 'cat.213.jpg', 'cat.463.jpg', 'cat.613.jpg', 'cat.307.jpg', 'cat.772.jpg', 'cat.412.jpg', 'cat.310.jpg', 'cat.685.jpg', 'cat.533.jpg', 'cat.958.jpg', 'cat.96.jpg', 'cat.479.jpg', 'cat.211.jpg', 'cat.474.jpg', 'cat.937.jpg', 'cat.46.jpg', 'cat.539.jpg', 'cat.567.jpg', 'cat.308.jpg', 'cat.779.jpg', 'cat.872.jpg', 'cat.108.jpg', 'cat.366.jpg', 'cat.725.jpg', 'cat.172.jpg', 'cat.517.jpg', 'cat.223.jpg', 'cat.367.jpg', 'cat.365.jpg', 'cat.416.jpg', 'cat.400.jpg', 'cat.760.jpg', 'cat.926.jpg', 'cat.6.jpg', 'cat.383.jpg', 'cat.893.jpg', 'cat.170.jpg', 'cat.917.jpg', 'cat.538.jpg', 'cat.616.jpg', 'cat.868.jpg', 'cat.311.jpg', 'cat.33.jpg', 'cat.629.jpg', 'cat.786.jpg', 'cat.801.jpg', 'cat.995.jpg', 'cat.670.jpg', 'cat.762.jpg', 'cat.656.jpg', 'cat.88.jpg', 'cat.476.jpg', 'cat.910.jpg', 'cat.65.jpg', 'cat.185.jpg', 'cat.523.jpg', 'cat.464.jpg', 'cat.272.jpg', 'cat.596.jpg', 'cat.427.jpg', 'cat.514.jpg', 'cat.203.jpg', 'cat.86.jpg', 'cat.301.jpg', 'cat.325.jpg', 'cat.831.jpg', 'cat.745.jpg', 'cat.347.jpg', 'cat.666.jpg', 'cat.933.jpg', 'cat.184.jpg', 'cat.663.jpg', 'cat.323.jpg', 'cat.85.jpg', 'cat.822.jpg', 'cat.396.jpg', 'cat.207.jpg', 'cat.309.jpg', 'cat.346.jpg', 'cat.52.jpg', 'cat.811.jpg', 'cat.836.jpg', 'cat.465.jpg', 'cat.957.jpg', 'cat.776.jpg', 'cat.369.jpg', 'cat.141.jpg', 'cat.370.jpg', 'cat.704.jpg', 'cat.190.jpg', 'cat.621.jpg', 'cat.478.jpg', 'cat.945.jpg', 'cat.743.jpg', 'cat.378.jpg', 'cat.561.jpg', 'cat.230.jpg', 'cat.534.jpg', 'cat.842.jpg', 'cat.907.jpg', 'cat.998.jpg', 'cat.778.jpg', 'cat.946.jpg', 'cat.397.jpg', 'cat.352.jpg', 'cat.574.jpg', 'cat.74.jpg', 'cat.458.jpg', 'cat.727.jpg', 'cat.183.jpg', 'cat.215.jpg', 'cat.282.jpg', 'cat.219.jpg', 'cat.975.jpg', 'cat.254.jpg', 'cat.709.jpg', 'cat.405.jpg', 'cat.562.jpg', 'cat.805.jpg', 'cat.455.jpg', 'cat.644.jpg', 'cat.449.jpg', 'cat.451.jpg', 'cat.205.jpg', 'cat.686.jpg', 'cat.919.jpg', 'cat.162.jpg', 'cat.467.jpg', 'cat.446.jpg', 'cat.477.jpg', 'cat.659.jpg', 'cat.227.jpg', 'cat.691.jpg', 'cat.300.jpg', 'cat.358.jpg', 'cat.258.jpg', 'cat.386.jpg', 'cat.903.jpg', 'cat.549.jpg', 'cat.101.jpg', 'cat.626.jpg', 'cat.883.jpg', 'cat.716.jpg', 'cat.232.jpg', 'cat.224.jpg', 'cat.712.jpg', 'cat.95.jpg', 'cat.794.jpg', 'cat.296.jpg', 'cat.880.jpg', 'cat.198.jpg', 'cat.882.jpg', 'cat.654.jpg', 'cat.990.jpg', 'cat.276.jpg', 'cat.251.jpg', 'cat.528.jpg', 'cat.20.jpg', 'cat.115.jpg', 'cat.878.jpg', 'cat.5.jpg', 'cat.793.jpg', 'cat.169.jpg', 'cat.577.jpg', 'cat.753.jpg', 'cat.941.jpg', 'cat.217.jpg', 'cat.555.jpg', 'cat.401.jpg', 'cat.418.jpg', 'cat.661.jpg', 'cat.98.jpg', 'cat.242.jpg', 'cat.989.jpg', 'cat.157.jpg', 'cat.780.jpg', 'cat.216.jpg', 'cat.714.jpg', 'cat.708.jpg', 'cat.445.jpg', 'cat.930.jpg', 'cat.813.jpg', 'cat.29.jpg', 'cat.631.jpg', 'cat.911.jpg', 'cat.769.jpg', 'cat.407.jpg', 'cat.982.jpg', 'cat.413.jpg', 'cat.75.jpg', 'cat.623.jpg', 'cat.662.jpg', 'cat.993.jpg', 'cat.295.jpg', 'cat.897.jpg', 'cat.123.jpg', 'cat.105.jpg', 'cat.457.jpg', 'cat.877.jpg', 'cat.863.jpg', 'cat.653.jpg', 'cat.137.jpg', 'cat.58.jpg', 'cat.110.jpg', 'cat.832.jpg', 'cat.484.jpg', 'cat.90.jpg', 'cat.602.jpg', 'cat.638.jpg', 'cat.703.jpg', 'cat.584.jpg', 'cat.163.jpg', 'cat.720.jpg', 'cat.861.jpg', 'cat.494.jpg', 'cat.856.jpg', 'cat.146.jpg', 'cat.604.jpg', 'cat.859.jpg', 'cat.235.jpg', 'cat.485.jpg', 'cat.138.jpg', 'cat.24.jpg', 'cat.13.jpg', 'cat.93.jpg', 'cat.197.jpg', 'cat.830.jpg', 'cat.3.jpg', 'cat.936.jpg', 'cat.668.jpg', 'cat.262.jpg', 'cat.576.jpg', 'cat.486.jpg', 'cat.660.jpg', 'cat.387.jpg', 'cat.56.jpg', 'cat.694.jpg', 'cat.80.jpg', 'cat.25.jpg', 'cat.876.jpg', 'cat.943.jpg', 'cat.874.jpg', 'cat.236.jpg', 'cat.432.jpg', 'cat.237.jpg', 'cat.335.jpg', 'cat.425.jpg', 'cat.554.jpg', 'cat.931.jpg', 'cat.304.jpg', 'cat.73.jpg', 'cat.147.jpg', 'cat.118.jpg', 'cat.143.jpg', 'cat.558.jpg', 'cat.91.jpg', 'cat.790.jpg', 'cat.782.jpg', 'cat.905.jpg', 'cat.356.jpg', 'cat.151.jpg', 'cat.200.jpg', 'cat.738.jpg', 'cat.78.jpg', 'cat.398.jpg', 'cat.38.jpg']\n"
          ]
        }
      ],
      "source": [
        "#this code block is useful for demonstrating a later point...\n",
        "\n",
        "os.chdir(train_dir)\n",
        "print(os.listdir())\n",
        "os.chdir('cats')\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAw6AR19sezV"
      },
      "outputs": [],
      "source": [
        "import numpy as np  #storing the data using this\n",
        "import cv2  #for reading the images into grayscale vectors\n",
        "from PIL import Image  #used to manipulate the images into the appropriate sizes\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTWkT0D4fFYV"
      },
      "source": [
        "# 2. Resizing the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgbW2Qb2kDHe",
        "outputId": "3e7c44b3-2046-415b-f711-4c454b6b99b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 0 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator as IDG  #keras's built in image augmentor, which we can use to specify image size and such\n",
        "\n",
        "size = (160, 160)  #the new size of each image after resizing. Dimensions are still up in the air\n",
        "\n",
        "#using IDG, make an augmentor with the parameters indicating what augmentations can take place.\n",
        "\n",
        "\n",
        "normal_generator = IDG(rescale=1./255)  #no need to have augmentation parameters since the model isn't being trained to fit data generated by this. \n",
        "                                        # Just normalize the data appropriately.\n",
        "\n",
        "#we'll use our augmented data generator instead of just extracting our training data. IDG has something perfect for this,\n",
        "#considering we have the directories already.\n",
        "\n",
        "#needed data generators\n",
        "training_generator = normal_generator.flow_from_directory(train_dir,\n",
        "                                                       target_size=size,  #force resizes all input images, super nice.\n",
        "                                                       batch_size=20,\n",
        "                                                       class_mode='binary',\n",
        "                                                       subset='training')  \n",
        "\n",
        "validation_generator = normal_generator.flow_from_directory(train_dir,\n",
        "                                                         target_size=size,\n",
        "                                                         batch_size=10,\n",
        "                                                         class_mode='binary',\n",
        "                                                         subset='validation')\n",
        "\n",
        "testing_generator = normal_generator.flow_from_directory(testing_dir,\n",
        "                                                         target_size=size,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode='binary')\n",
        "\n",
        "#This is why we have the data separated into two subdirectories in the training and validation directories.\n",
        "#The way the data can tell the cats from the dogs is simply due to the fact that they are in two different\n",
        "#sub directories as they flow in from their main directories. The generator automatically applies labels to\n",
        "#them this way. Super useful. And since validation and training have the same subdirectories, the labels will\n",
        "#be the same as well, which is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0_5stCLfIKY"
      },
      "source": [
        "# 3. Creating the neural network model (cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xdoioiPJdAh",
        "outputId": "37ce9851-5ded-414c-e685-eac1205ad573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 158, 158, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 79, 79, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 77, 77, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 38, 38, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 36, 36, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 18, 18, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 8, 8, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              16778240  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,167,681\n",
            "Trainable params: 17,167,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential as seq\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten,  MaxPooling2D, Dropout\n",
        "#Standard model creation that can be used for most projects. You can play around with layers if you would like to increase or decrease accuracy.\n",
        "def createModel():\n",
        "  model = seq()\n",
        "  model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(size[0],size[1],3)))                                                                                       #The 3 indicates RGB, since we don't need to grayscale.\n",
        "  model.add(MaxPooling2D((2,2)))  #reduces dimensionality for computational ease\n",
        "  \n",
        "  model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "  \n",
        "  model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "  \n",
        "  model.add(Conv2D(256, kernel_size=3, activation='relu'))\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "  \n",
        "  model.add(Dropout(0.5))\n",
        "  \n",
        "  model.add(Flatten())  #allows the data from the previous layer to be fed into a standard dense layer\n",
        "  \n",
        "  #the rest of these is architecture of the normal deep neural network\n",
        "\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid')) \n",
        "  \n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "myModel = createModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2esw6V6MSd2"
      },
      "source": [
        "Note that recall and precision, in this case, are not very important. The errors of identifying a cat as a dog and vice versa are of equal consequence; thus, there is no real reason to check for recall and precision, and relying on accuracy is a good enough metric to judge our CNN. Based on this, we can now compile and train our model with accuracy as its sole metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2KPV7BfKxX"
      },
      "source": [
        "# 4. Training the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OezUAlvIXkAa",
        "outputId": "b040f679-9d0d-4e96-c575-6cb948e30e2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "80/80 [==============================] - 114s 1s/step - loss: 0.6999 - accuracy: 0.5337\n",
            "Epoch 2/100\n",
            "80/80 [==============================] - 111s 1s/step - loss: 0.6344 - accuracy: 0.6331\n",
            "Epoch 3/100\n",
            "80/80 [==============================] - 110s 1s/step - loss: 0.5942 - accuracy: 0.6762\n",
            "Epoch 4/100\n",
            "80/80 [==============================] - 110s 1s/step - loss: 0.5532 - accuracy: 0.7150\n",
            "Epoch 5/100\n",
            "80/80 [==============================] - 111s 1s/step - loss: 0.5371 - accuracy: 0.7337\n",
            "Epoch 6/100\n",
            "80/80 [==============================] - 110s 1s/step - loss: 0.5118 - accuracy: 0.7462\n",
            "Epoch 7/100\n",
            "80/80 [==============================] - 110s 1s/step - loss: 0.4770 - accuracy: 0.7613\n",
            "Epoch 8/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.4388 - accuracy: 0.7987\n",
            "Epoch 9/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.4293 - accuracy: 0.7987\n",
            "Epoch 10/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.3884 - accuracy: 0.8250\n",
            "Epoch 11/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.3561 - accuracy: 0.8431\n",
            "Epoch 12/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.3315 - accuracy: 0.8600\n",
            "Epoch 13/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.3110 - accuracy: 0.8681\n",
            "Epoch 14/100\n",
            "80/80 [==============================] - 110s 1s/step - loss: 0.2824 - accuracy: 0.8750\n",
            "Epoch 15/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.2646 - accuracy: 0.8869\n",
            "Epoch 16/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.2449 - accuracy: 0.9050\n",
            "Epoch 17/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.2300 - accuracy: 0.9062\n",
            "Epoch 18/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.2048 - accuracy: 0.9162\n",
            "Epoch 19/100\n",
            "80/80 [==============================] - 109s 1s/step - loss: 0.1897 - accuracy: 0.9194\n",
            "Epoch 20/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.1600 - accuracy: 0.9381\n",
            "Epoch 21/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.1644 - accuracy: 0.9325\n",
            "Epoch 22/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.1343 - accuracy: 0.9506\n",
            "Epoch 23/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.1350 - accuracy: 0.9444\n",
            "Epoch 24/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.1078 - accuracy: 0.9563\n",
            "Epoch 25/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.1167 - accuracy: 0.9506\n",
            "Epoch 26/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.1115 - accuracy: 0.9563\n",
            "Epoch 27/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0909 - accuracy: 0.9656\n",
            "Epoch 28/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0987 - accuracy: 0.9613\n",
            "Epoch 29/100\n",
            "80/80 [==============================] - 102s 1s/step - loss: 0.0747 - accuracy: 0.9712\n",
            "Epoch 30/100\n",
            "80/80 [==============================] - 102s 1s/step - loss: 0.0934 - accuracy: 0.9644\n",
            "Epoch 31/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0741 - accuracy: 0.9769\n",
            "Epoch 32/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0686 - accuracy: 0.9769\n",
            "Epoch 33/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0769 - accuracy: 0.9725\n",
            "Epoch 34/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0555 - accuracy: 0.9850\n",
            "Epoch 35/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0640 - accuracy: 0.9769\n",
            "Epoch 36/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0483 - accuracy: 0.9794\n",
            "Epoch 37/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0633 - accuracy: 0.9775\n",
            "Epoch 38/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0742 - accuracy: 0.9737\n",
            "Epoch 39/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0419 - accuracy: 0.9837\n",
            "Epoch 40/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0580 - accuracy: 0.9756\n",
            "Epoch 41/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0432 - accuracy: 0.9819\n",
            "Epoch 42/100\n",
            "80/80 [==============================] - 102s 1s/step - loss: 0.0421 - accuracy: 0.9869\n",
            "Epoch 43/100\n",
            "80/80 [==============================] - 102s 1s/step - loss: 0.0639 - accuracy: 0.9769\n",
            "Epoch 44/100\n",
            "80/80 [==============================] - 101s 1s/step - loss: 0.0433 - accuracy: 0.9825\n",
            "Epoch 45/100\n",
            "80/80 [==============================] - 103s 1s/step - loss: 0.0492 - accuracy: 0.9831\n",
            "Epoch 46/100\n",
            "80/80 [==============================] - 103s 1s/step - loss: 0.0395 - accuracy: 0.9881\n",
            "Epoch 47/100\n",
            "80/80 [==============================] - 103s 1s/step - loss: 0.0426 - accuracy: 0.9825\n",
            "Epoch 48/100\n",
            "80/80 [==============================] - 103s 1s/step - loss: 0.0349 - accuracy: 0.9856\n",
            "Epoch 49/100\n",
            "80/80 [==============================] - 103s 1s/step - loss: 0.0320 - accuracy: 0.9900\n",
            "Epoch 50/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0479 - accuracy: 0.9825\n",
            "Epoch 51/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0367 - accuracy: 0.9869\n",
            "Epoch 52/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0460 - accuracy: 0.9862\n",
            "Epoch 53/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0340 - accuracy: 0.9875\n",
            "Epoch 54/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0353 - accuracy: 0.9856\n",
            "Epoch 55/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0387 - accuracy: 0.9869\n",
            "Epoch 56/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0378 - accuracy: 0.9869\n",
            "Epoch 57/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0455 - accuracy: 0.9844\n",
            "Epoch 58/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0426 - accuracy: 0.9875\n",
            "Epoch 59/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0388 - accuracy: 0.9844\n",
            "Epoch 60/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0278 - accuracy: 0.9887\n",
            "Epoch 61/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0366 - accuracy: 0.9875\n",
            "Epoch 62/100\n",
            "80/80 [==============================] - 109s 1s/step - loss: 0.0306 - accuracy: 0.9881\n",
            "Epoch 63/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0367 - accuracy: 0.9894\n",
            "Epoch 64/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0186 - accuracy: 0.9919\n",
            "Epoch 65/100\n",
            "80/80 [==============================] - 109s 1s/step - loss: 0.0335 - accuracy: 0.9875\n",
            "Epoch 66/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0399 - accuracy: 0.9862\n",
            "Epoch 67/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0214 - accuracy: 0.9906\n",
            "Epoch 68/100\n",
            "80/80 [==============================] - 109s 1s/step - loss: 0.0253 - accuracy: 0.9906\n",
            "Epoch 69/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0253 - accuracy: 0.9919\n",
            "Epoch 70/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0294 - accuracy: 0.9900\n",
            "Epoch 71/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0281 - accuracy: 0.9931\n",
            "Epoch 72/100\n",
            "80/80 [==============================] - 107s 1s/step - loss: 0.0315 - accuracy: 0.9906\n",
            "Epoch 73/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0250 - accuracy: 0.9937\n",
            "Epoch 74/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0215 - accuracy: 0.9919\n",
            "Epoch 75/100\n",
            "80/80 [==============================] - 108s 1s/step - loss: 0.0286 - accuracy: 0.9894\n",
            "Epoch 76/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0231 - accuracy: 0.9900\n",
            "Epoch 77/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0308 - accuracy: 0.9881\n",
            "Epoch 78/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0338 - accuracy: 0.9887\n",
            "Epoch 79/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0256 - accuracy: 0.9912\n",
            "Epoch 80/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0193 - accuracy: 0.9912\n",
            "Epoch 81/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0200 - accuracy: 0.9931\n",
            "Epoch 82/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0233 - accuracy: 0.9931\n",
            "Epoch 83/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0275 - accuracy: 0.9919\n",
            "Epoch 84/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0189 - accuracy: 0.9925\n",
            "Epoch 85/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0260 - accuracy: 0.9925\n",
            "Epoch 86/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0174 - accuracy: 0.9944\n",
            "Epoch 87/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0431 - accuracy: 0.9900\n",
            "Epoch 88/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0164 - accuracy: 0.9944\n",
            "Epoch 89/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0357 - accuracy: 0.9912\n",
            "Epoch 90/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0216 - accuracy: 0.9950\n",
            "Epoch 91/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0244 - accuracy: 0.9937\n",
            "Epoch 92/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0268 - accuracy: 0.9925\n",
            "Epoch 93/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0208 - accuracy: 0.9937\n",
            "Epoch 94/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0213 - accuracy: 0.9900\n",
            "Epoch 95/100\n",
            "80/80 [==============================] - 104s 1s/step - loss: 0.0209 - accuracy: 0.9937\n",
            "Epoch 96/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0145 - accuracy: 0.9937\n",
            "Epoch 97/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0243 - accuracy: 0.9919\n",
            "Epoch 98/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0282 - accuracy: 0.9919\n",
            "Epoch 99/100\n",
            "80/80 [==============================] - 105s 1s/step - loss: 0.0321 - accuracy: 0.9919\n",
            "Epoch 100/100\n",
            "80/80 [==============================] - 106s 1s/step - loss: 0.0225 - accuracy: 0.9950\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop  #special kind of optimizer that allows \n",
        "\n",
        "myModel.compile(loss='binary_crossentropy',\n",
        "                optimizer=RMSprop(lr=2e-4),  #learning rate of 0.0002 originally\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "t_history = myModel.fit_generator(training_generator, \n",
        "                                  validation_data=validation_generator,\n",
        "                                  validation_steps=40,\n",
        "                                  epochs=100,\n",
        "                                  steps_per_epoch=80)\n",
        "                                \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEz2HXd-fOZc"
      },
      "source": [
        "# 5. Checking our model's stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0xIVuc7XGHHT",
        "outputId": "fd83cf37-0b49-4828-93ce-b979096661a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing loss: 1.841797113418579\n",
            "Testing accuracy: 0.7720000147819519\n"
          ]
        }
      ],
      "source": [
        " testing_loss, testing_acc = myModel.evaluate_generator(testing_generator)\n",
        "#just for you to see loss and accuracy(Note: these are decimals out of 1)\n",
        "print(\"Testing loss: {}\".format(testing_loss))\n",
        "print(\"Testing accuracy: {}\".format(testing_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWdk_xMVfQZg"
      },
      "source": [
        "# 6. Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cMXRxigGjbrK"
      },
      "outputs": [],
      "source": [
        "#make a directory for saving models to\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "os.mkdir(\"saved_models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M_VQhcM-sCkD"
      },
      "outputs": [],
      "source": [
        "#save the current model to a file\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "os.chdir(\"saved_models\")\n",
        "model_num = 3  #change this manually after the run. this was the best model we had\n",
        "myModel.save(\"model_{}.h5\".format(model_num))  #remember to download it locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfcPC8OKfSjd"
      },
      "source": [
        "# 7. Try out the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bFgfeY_WOtLC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "def getImage2(url):  \n",
        "  im = Image.open(requests.get(url, stream=True).raw)\n",
        "  return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mJn_VcU7htuT",
        "outputId": "28d64af4-8ba7-4ace-b6c3-abedce8297a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If you give me an image, I'll try to classify it as a dog or a cat!\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"If you give me an image, I'll try to classify it as a dog or a cat!\")\n",
        "img_url = str(input(\"Enter the image URL:\"))\n",
        "retr_img = getImage2(img_url)\n",
        "plt.imshow(retr_img)\n",
        "\n",
        "#adjust the image appropriately\n",
        "retr_img = retr_img.resize(size)\n",
        "retr_img = np.divide(np.array(retr_img),255)\n",
        "\n",
        "retr_img = retr_img.reshape(1,size[0],size[1],3)\n",
        "\n",
        "#print the prediction!\n",
        "print(\"Classified as cat.\") if myModel.predict(retr_img)[0][0] <= 0.5 else print(\"Classified as dog.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cats_V_Dogs_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
